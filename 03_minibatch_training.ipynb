{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_02 import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data,input_dim = x_train.shape\n",
    "n_class = y_train.max()+1\n",
    "hid_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(input_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, output_dim)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x=l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dim, hid_dim, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 784])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0382, -0.1076, -0.0844,  ...,  0.0555,  0.0768, -0.0209],\n",
       "        [-0.0924, -0.1169,  0.0004,  ...,  0.0489,  0.0562, -0.0237],\n",
       "        [-0.0640, -0.0548, -0.0828,  ...,  0.0403,  0.0469,  0.0179],\n",
       "        ...,\n",
       "        [-0.0542, -0.2227, -0.0232,  ...,  0.0431,  0.0271, -0.1417],\n",
       "        [-0.0519, -0.1675, -0.0636,  ...,  0.0273,  0.0011,  0.0627],\n",
       "        [-0.0691, -0.1988, -0.1204,  ...,  0.0672,  0.0684, -0.0464]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy loss\n",
    "\n",
    "First, we will need to compute the `softmax` of our activations. This is defined by:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
    "\n",
    "or more concisely:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$\n",
    "\n",
    "In practice, we will need the **log of the softmax** when we calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return (x.exp()/x.exp().sum(dim=-1,keepdim=True)).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_pred = log_softmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3615, -2.4309, -2.4077,  ..., -2.2678, -2.2465, -2.3441],\n",
       "        [-2.4189, -2.4434, -2.3261,  ..., -2.2776, -2.2703, -2.3502],\n",
       "        [-2.3979, -2.3887, -2.4167,  ..., -2.2936, -2.2870, -2.3160],\n",
       "        ...,\n",
       "        [-2.3406, -2.5091, -2.3096,  ..., -2.2433, -2.2593, -2.4281],\n",
       "        [-2.3698, -2.4853, -2.3814,  ..., -2.2906, -2.3167, -2.2551],\n",
       "        [-2.3648, -2.4945, -2.4161,  ..., -2.2286, -2.2274, -2.3422]],\n",
       "       grad_fn=<LogBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target): \n",
    "    \n",
    "    return -input[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nll(sm_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3170, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The formula\n",
    "\n",
    "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$\n",
    "\n",
    "gives a simplification when we compute the log softmax, which was previously defined as \n",
    "\n",
    "```py\n",
    "(x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(nll(log_softmax(pred), y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, there is a way to compute the `log` of the sum of exponentials in a more stable way, called the `LogSumExp` trick. The idea is to use the following formula:\n",
    "\n",
    "$$\n",
    "\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )\n",
    "$$\n",
    "\n",
    "where a is the maximum of the $x_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    m = x.max(1)[0]\n",
    "    return m + (x-m[:,None]).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(logsumexp(pred), pred.logsumexp(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(nll(log_softmax(pred), y_train), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(F.nll_loss(F.log_softmax(pred, -1), y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, `F.log_softmax` and `F.nll_loss` are combined in one optimized function, `F.cross_entropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(F.cross_entropy(pred, y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training loop\n",
    "Basically the training loop repeats over the following steps:\n",
    "\n",
    "- Get the output of the model on a batch of inputs\n",
    "- Compare the output to the labels we have and compute a loss\n",
    "- Calculate the gradients of the loss with respect to every parameter of the model\n",
    "- Update said parameters with those gradients to make them a little bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def accuracy(out: torch.Tensor, yb:torch.Tensor) -> torch.Tensor:\n",
    "    return (torch.argmax(out, dim=1)==yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0382, -0.1076, -0.0844,  0.1018,  0.1022,  0.1481, -0.0628,  0.0555,\n",
       "          0.0768, -0.0209], grad_fn=<SelectBackward>), torch.Size([64, 10]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sz = 64\n",
    "\n",
    "xb = x_train[:batch_sz]\n",
    "yb = y_train[:batch_sz]\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3260, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0625)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5   # learning rate\n",
    "epochs = 2 # how many epochs to train for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tqdm tweak:**\n",
    "\n",
    "I have modified the code of jeremy using a cool `tqdm` tweak from [Kaggle-Very Simple Pytorch Training by Abhishek Thakur](https://www.kaggle.com/abhishek/very-simple-pytorch-training-0-59)\n",
    "\n",
    "- The tweak helps to print the running loss inside the `tqdm` progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4637cdb5484d77b65e3f37da33363c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 20.5814\n",
      "==========\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0971ba33032b4497a037403c8a089fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 9.3083\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    \n",
    "    data_range = (n_data-1)// batch_sz + 1\n",
    "    \n",
    "    tk0 = tqdm.tqdm_notebook(range(data_range))\n",
    "    \n",
    "    for i in tk0:\n",
    "        start_i = i*batch_sz\n",
    "        end_i = start_i + batch_sz\n",
    "        \n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        \n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, 'weight'):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias   -= l.bias.grad   * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias  .grad.zero_()\n",
    "                    \n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        counter += 1\n",
    "        tk0.set_postfix(loss=(running_loss / (counter * batch_sz)))\n",
    "        \n",
    "    epoch_loss = running_loss / data_range\n",
    "    print('Training Loss: {:.4f}'.format(epoch_loss))\n",
    "    print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0096, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using parameters and optim\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "Use `nn.Module.__setattr__` and move `relu` to functional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(input_dim, hid_dim)\n",
    "        self.l2 = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "    def __call__(self, x): return self.l2(F.relu(self.l1(x)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dim, hid_dim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1: Linear(in_features=784, out_features=50, bias=True)\n",
      "l2: Linear(in_features=50, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_children(): print(f\"{name}: {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (l2): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        data_range = (n_data-1)// batch_sz + 1\n",
    "        tk0 = tqdm.tqdm_notebook(range(data_range))\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        counter = 0\n",
    "        \n",
    "        for i in tk0:\n",
    "            start_i = i*batch_sz\n",
    "            end_i = start_i+batch_sz\n",
    "            \n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "        \n",
    "            preds = model(xb)\n",
    "            loss = loss_func(preds, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters(): p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "                \n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            counter += 1\n",
    "            tk0.set_postfix(loss=(running_loss / (counter * batch_sz)))\n",
    "        \n",
    "        epoch_loss = running_loss / data_range\n",
    "        print('Training Loss: {:.4f}'.format(epoch_loss))\n",
    "        print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the difference between `torch.no_grad()` and `torch.set_grad_enabled()` ??\n",
    "\n",
    "[Question](https://discuss.pytorch.org/t/confused-about-set-grad-enabled/38417)\n",
    "\n",
    "1. What does the part\n",
    "\n",
    "```py\n",
    "with torch.set_grad_enabled(phase == 'train'):\n",
    "```\n",
    "\n",
    "do? I thought that `model.eval()` and `model.train()` is enough to put the model into states in which to save and evaluate backprob information.\n",
    "\n",
    "2. How does it compare to `requires_grad()` ? do the fulfill the same purpose?\n",
    "\n",
    "3. Why is the epoch_loss divided by the size of the training set? According to the docs the loss is by default averaged over the batch size (reduction=‘mean’), so why would I divide by the entire data set size in the end?\n",
    "\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "1. `model.train()` and `model.eval()` change the behavior of some layers. E.g. `nn.Dropout` won’t drop anymore and `nn.BatchNorm` layers will use the running estimates instead of the batch statistics. The `torch.set_grad_enabled` line of code makes sure to clear the intermediate values for evaluation, which are needed to backpropagate during training, thus saving memory. It’s comparable to the with `torch.no_grad()` statement but takes a bool value.\n",
    "\n",
    "2. All new operations in the `torch.set_grad_enabled(False)` block won’t require gradients. However, the model parameters will still require gradients.\n",
    "\n",
    "3. The running_loss will be `de-averaged` by multiplying it with `xb.size(0)`. Therefore you should divide by the whole dataset length, not the number of batches.\n",
    "\n",
    "\n",
    "\n",
    "Regarding question 3:\n",
    "\n",
    "- Q. Why would one de-average the loss only to divide later by the entire data set size? for smoothing reasons of the results? Can the de-averaging also be done by omitting the multiplication with `xb.size(0)` and setting the reduction parameter to ‘none’ in the loss function initialization?\n",
    "\n",
    "- Ans. If your dataset length is not divisible by the batch size, the last batch will contain less samples than all others. Thus taking the averaged loss and divide it by the number of batches will give a a slightly biased result.\n",
    "\n",
    "- You could use sum to get the accumulated loss for debugging purposes. However, I would still recommend to take the mean for the loss for backpropagation, since otherwise e.g. the learning rate will depend on the batch size.\n",
    "\n",
    "\n",
    "\n",
    "Reference:\n",
    "- [Use of set_grad_enable()](https://www.kaggle.com/abhishek/very-simple-pytorch-training-0-59)\n",
    "- [confused about set_grad_enabled](https://discuss.pytorch.org/t/confused-about-set-grad-enabled/38417/4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `__setattr__`\n",
    "\n",
    "Behind the scenes, PyTorch overrides the `__setattr__` function in `nn.Module` so that the submodules you define are properly registered as parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModule():\n",
    "    def __init__(self, input_dim, hid_dim, output_dim):\n",
    "        self._modules = {}\n",
    "        self.l1 = nn.Linear(input_dim,hid_dim)\n",
    "        self.l2 = nn.Linear(hid_dim,output_dim)\n",
    "        \n",
    "    def __setattr__(self,k,v):\n",
    "        if not k.startswith(\"_\"): self._modules[k] = v\n",
    "        super().__setattr__(k,v)\n",
    "        \n",
    "    def __repr__(self): return f'{self._modules}'\n",
    "    \n",
    "    def parameters(self):\n",
    "        for l in self._modules.values():\n",
    "            for p in l.parameters(): yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = DummyModule(input_dim,hid_dim,10)\n",
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([50, 784]),\n",
       " torch.Size([50]),\n",
       " torch.Size([10, 50]),\n",
       " torch.Size([10])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.shape for o in mdl.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering modules\n",
    "\n",
    "We can use the original layers approach, but we have to register the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(input_dim,hid_dim), nn.ReLU(), nn.Linear(hid_dim,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer_1): ReLU()\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.ModuleList\n",
    "\n",
    "`nn.ModuleList` does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers: x=layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(input_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequentialModel(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1c1ff02a574c6a8bc6b5a48020ac53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 19.6519\n",
      "==========\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b025c32e0b9f42c29f96834cdc561dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 8.8945\n",
      "==========\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0200, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Sequential\n",
    "\n",
    "`nn.Sequential` is a convenient class which does the same as the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(input_dim,hid_dim), nn.ReLU(), nn.Linear(hid_dim,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a678ff2209440de9bce95e17443f195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 20.5509\n",
      "==========\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925838dd77204d91861d4f1ae9a3d21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 9.6941\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optim\n",
    "\n",
    "Let's replace our previous manually coded optimization step:\n",
    "\n",
    "```py\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()\n",
    "```\n",
    "\n",
    "and instead use just:\n",
    "\n",
    "```py\n",
    "opt.step()  #Update the params\n",
    "opt.zero_grad() #Reset param grad value to zero\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Optimizer should take all the `model parameters` as input\n",
    "\n",
    "```py\n",
    "def __init__(self, params, lr:float=0.5):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "```\n",
    "\n",
    "Remember `model.parameters()` returns a `generator`. So in the `__init__()` of the `class Optimizer`, apply `list` over the `generator` passed as argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr:float=0.5):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Update the model parameters\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * lr         \n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Reset the model params to zero\n",
    "        \"\"\"\n",
    "        for p in self.params: p.grad.data.zero_()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.no_grad??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(input_dim,hid_dim), nn.ReLU(), nn.Linear(hid_dim,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit2()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit2(epochs:int=2):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        data_range = (n_data-1)// batch_sz + 1\n",
    "        tk0 = tqdm.tqdm_notebook(range(data_range))\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        counter = 0\n",
    "        \n",
    "        for i in tk0:\n",
    "            start_i = i*batch_sz\n",
    "            end_i = start_i+batch_sz\n",
    "            \n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "        \n",
    "            preds = model(xb)\n",
    "            loss = loss_func(preds, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            counter += 1\n",
    "            tk0.set_postfix(loss=(running_loss / (counter * batch_sz)))\n",
    "        \n",
    "        epoch_loss = running_loss / data_range\n",
    "        print('Training Loss: {:.4f}'.format(epoch_loss))\n",
    "        print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8bf292a30849fa917f9aebfa927631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 20.1380\n",
      "==========\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6fb4eb47ad4b7e86ad93dede9a5c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 9.0100\n",
      "==========\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8bf8d3ae4749969a9d7370b5898620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 6.8189\n",
      "==========\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc1986ac40b48a1ad965dcf79ee0465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 5.6133\n",
      "==========\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da13ffa0cbfb4e9888b3ffeae429237d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.7610\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "fit2(epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0041, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** PyTorch already provides this exact functionality in `optim.SGD` (it also handles stuff like `momentum`, which we'll look at later - except we'll be doing it in a more flexible way!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim.SGD.step??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(input_dim,hid_dim), nn.ReLU(), nn.Linear(hid_dim,10))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2983, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "loss_func(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed81fa8887874f01828ae6603c2ed0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 6.6328\n",
      "==========\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639aa99a05d843c693c7870a1fd342b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 5.3295\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "fit2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0276, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Randomized tests can be very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc>0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "\n",
    "It's clunky to iterate through minibatches of x and y values separately:\n",
    "\n",
    "```py\n",
    "    xb = x_train[start_i:end_i]\n",
    "    yb = y_train[start_i:end_i]\n",
    "```\n",
    "\n",
    "Instead, let's do these two steps together, by introducing a Dataset class:\n",
    "\n",
    "```py\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "```\n",
    "\n",
    "The `Dataset` class has\n",
    "\n",
    "- `__init__`: takes raw tensor `x_train`, `y_train`\n",
    "- `__len__`: Return length of the dataset\n",
    "- `__getitem__(slice)`: It takes `Slice()` as input `index_range` and returns subset of data in that range\n",
    "\n",
    "In the below code snippet\n",
    "```py\n",
    "xb,yb = train_ds[0:5]\n",
    "```\n",
    "\n",
    "`[0:5]` converts to `Slice(0,5)` by default in the `dunder` method `__getitem__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset():\n",
    "    \n",
    "    def __init__(self, x:torch.Tensor, y:torch.Tensor):\n",
    "        self.x , self.y = x, y\n",
    "        \n",
    "    def __len__(self): return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx_range:slice):\n",
    "        # print(f\"index: {index}\")\n",
    "        return self.x[idx_range], self.y[idx_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
    "assert len(train_ds)==len(x_train)\n",
    "assert len(valid_ds)==len(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = train_ds[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slice(0, 5, None)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = train_ds[0:5]\n",
    "assert xb.shape==(5,28*28)\n",
    "assert yb.shape==(5,)\n",
    "xb,yb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit3()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit3(epochs:int=2):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        data_range = (n_data-1)// batch_sz + 1\n",
    "        tk0 = tqdm.tqdm_notebook(range(data_range))\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        counter = 0\n",
    "        \n",
    "        for i in tk0:\n",
    "            xb,yb = train_ds[i*batch_sz : i*batch_sz+batch_sz]\n",
    "        \n",
    "            preds = model(xb)\n",
    "            loss = loss_func(preds, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            counter += 1\n",
    "            tk0.set_postfix(loss=(running_loss / (counter * batch_sz)))\n",
    "        \n",
    "        epoch_loss = running_loss / data_range\n",
    "        print('Training Loss: {:.4f}'.format(epoch_loss))\n",
    "        print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "825b6e7aec4c48b79281642be1c19a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 20.1996\n",
      "==========\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12c85556f3047778e4e2a3822434cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 9.2264\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "fit3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0087, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "Previously, our loop iterated over batches `(xb, yb)` like this:\n",
    "\n",
    "```py\n",
    "for i in range((n-1)//bs + 1):\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "    ...\n",
    "```\n",
    "\n",
    "Let's make our loop much cleaner, using a `data loader`:\n",
    "\n",
    "```py\n",
    "for xb,yb in train_dl:\n",
    "```\n",
    "\n",
    "**Note:** As you can understand to get the above `train_dl` we need to use the `yield` from python \n",
    "inside `dunder` method `__iter__`\n",
    "\n",
    "The `DataLoader` class has:\n",
    "\n",
    "- `__init__()`: takes an object of `class Dataset` and `batch_size`\n",
    "- `__iter__`: `yelds` dataset by appropriate `Slice()` using the `batch_size` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset: Dataset, batch_sz:int):\n",
    "        self.dataset = dataset\n",
    "        self.batch_sz = batch_sz\n",
    "     \n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.batch_sz):\n",
    "            # the 3rd argument in range() is the step to skip\n",
    "            yield self.dataset[i:i+self.batch_sz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_sz)\n",
    "valid_dl = DataLoader(valid_ds, batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(valid_dl))\n",
    "assert xb.shape==(batch_sz,28*28)\n",
    "assert yb.shape==(batch_sz,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'True Label: 3')"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[0].view(28,28))\n",
    "plt.title(f\"True Label: {yb[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit4()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit4(epochs:int=2):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        counter = 0\n",
    "        \n",
    "        for xb, yb in train_dl:\n",
    "        \n",
    "            preds = model(xb)\n",
    "            loss = loss_func(preds, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            counter += 1\n",
    "            \n",
    "        \n",
    "        epoch_loss = running_loss / data_range\n",
    "        print('Training Loss: {:.4f}'.format(epoch_loss))\n",
    "        print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "Training Loss: 21.1936\n",
      "==========\n",
      "Epoch 1/1\n",
      "Training Loss: 10.0820\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "fit4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0327, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweak `tqdm`\n",
    "\n",
    "Let's tweak the `fit4()` using `tqdm` like this [kaggle kernel](https://www.kaggle.com/abhishek/very-simple-pytorch-training-0-59) which is helpful.\n",
    "\n",
    "#### `fit5()`\n",
    "\n",
    "**Note:** Tweaking `tqdm` into `fit4()` we obtain the following `fit5()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 64, 782.25, 782)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batches = (n_data-1)// batch_sz + 1\n",
    "n_data, batch_sz, n_data/batch_sz+1, n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit5(epochs:int=2):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        counter = 0\n",
    "        \n",
    "        n_batches =  (len(train_dl.dataset)-1)// batch_sz + 1 \n",
    "        \n",
    "        # creating tqdm object\n",
    "        tk0 = tqdm.tqdm_notebook(train_dl, total=n_batches)\n",
    "        \n",
    "        for xb, yb in tk0:\n",
    "        \n",
    "            preds = model(xb)\n",
    "            loss = loss_func(preds, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            counter += 1\n",
    "            tk0.set_postfix(loss=(running_loss / (counter * train_dl.batch_sz)))\n",
    "        \n",
    "        epoch_loss = running_loss / data_range\n",
    "        print('Training Loss: {:.4f}'.format(epoch_loss))\n",
    "        print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "- Q. Why `len(train_dl.dataset)` returns the length of the dataset object from `class Dataset`\n",
    "- A. In `class Dataset` we implemented the `dunder` method `__len__` which overwrite the `len()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc5da1c55bd49788ffc5cdf38014fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.8867\n",
      "==========\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7089629c837c43f592dae4d23335c57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.6843\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "fit5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random DataLoader: Sampling\n",
    "\n",
    "- We want our training set to be in a random order, and that order should differ each iteration. \n",
    "- But the _validation set shouldn't be randomized_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    \"\"\"\n",
    "    Random permutation of the dataset indices based on the flag: shuffle\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: Dataset, batch_sz:int, shuffle:bool=False):\n",
    "        self.dataset_sz = len(dataset)\n",
    "        self.batch_sz = batch_sz\n",
    "        self.shuffle = shuffle\n",
    "     \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Returns: \n",
    "                a batch of randomized indices, \n",
    "        \n",
    "        which are used in the dataloader to pick the original values \n",
    "        \"\"\"\n",
    "        \n",
    "        if self.shuffle:\n",
    "            self.idxs = torch.randperm(self.dataset_sz) \n",
    "        else: \n",
    "            self.idxs = torch.arange(self.dataset_sz)\n",
    "        \n",
    "        for i in range(0, self.dataset_sz, self.batch_sz):\n",
    "            yield self.idxs[i:i+self.batch_sz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ds = Dataset(*train_ds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling WITHOUT shuffle. Initial order of indices in the chunk of batch size\n",
    "s = Sampler(small_ds,3,False)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([8, 6, 7]), tensor([4, 1, 5]), tensor([9, 3, 0]), tensor([2])]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling WITH shuffle. Random order of indices in the chunk of batch size\n",
    "s = Sampler(small_ds,3,True)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "\n",
    "- What is the purpose of `collate()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 7, 0])\n",
      "(tensor([4, 3, 5]),)\n",
      "tensor([9, 3, 8])\n",
      "(tensor([4, 1, 1]),)\n",
      "tensor([1, 5, 4])\n",
      "(tensor([0, 2, 9]),)\n",
      "tensor([6])\n",
      "(tensor([1]),)\n"
     ]
    }
   ],
   "source": [
    "for o in s:\n",
    "    print(o)\n",
    "    b = small_ds[o]\n",
    "    xbt, ybt = zip(b)\n",
    "    print(ybt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    xs,ys = zip(*b)\n",
    "    return torch.stack(xs),torch.stack(ys)\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds: Dataset, sampler: Sampler, collate_fn=collate):\n",
    "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "257.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
