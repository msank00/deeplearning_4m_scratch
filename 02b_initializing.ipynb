{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why you need a good init\n",
    "\n",
    "To understand why initialization is important in a neural net, we'll focus on the basic operation you have there: matrix multiplications. So let's just take a vector x, and a matrix a initialized randomly, then multiply them 100 times (as if we had 100 layers).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100): x = a @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(x: torch.Tensor)->torch.Tensor:\n",
    "    print(f\"mean:  {x.mean()}, std: {x.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  nan, std: nan\n"
     ]
    }
   ],
   "source": [
    "stats(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Explosion\n",
    "\n",
    "\n",
    "The problem you'll get with that is `activation explosion`: very soon, your activations will go to `nan`. We can even ask the loop to break when that first happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512)\n",
    "\n",
    "for i in range(100): \n",
    "    x = a @ x\n",
    "    #if x.std() != x.std(): break\n",
    "    if torch.isnan(x.std()): break\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to check if a torch.Tensor is nan?**\n",
    "\n",
    "You can always leverage the fact that `nan != nan`\n",
    "\n",
    "```py\n",
    ">>> x = torch.tensor([1, 2, np.nan])\n",
    "tensor([  1.,   2., nan.])\n",
    ">>> x != x\n",
    "tensor([ 0,  0,  1], dtype=torch.uint8)\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```py\n",
    ">>> torch.isnan(x)\n",
    "tensor([ 0,  0,  1], dtype=torch.uint8)\n",
    "```\n",
    "\n",
    "- [Ref:StackOverflow](https://stackoverflow.com/questions/48158017/pytorch-operation-to-detect-nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  nan, std: nan\n"
     ]
    }
   ],
   "source": [
    "stats(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It only takes around 30 multiplications! On the other hand, if you initialize your activations with a scale that is too low, then you'll get another problem.\n",
    "\n",
    "## Vanishing Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  0.0, std: 0.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512) * 0.01\n",
    "\n",
    "for i in range(100): x = a @ x\n",
    "\n",
    "stats(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, every activation vanished to 0. So to avoid that problem, people have come with several strategies to initialize their weight matrices.\n",
    "\n",
    "\n",
    "## Initialization Strategies \n",
    "\n",
    "- Use a standard deviation that will make sure `x` and `Ax` have exactly the same scale\n",
    "- Use an `orthogonal matrix` to initialize the weight (orthogonal matrices have the special property that they preserve the `L2` norm, so `x` and `Ax` would have the same sum of squares in that case)\n",
    "- Use `spectral normalization` on the matrix `A` (the spectral norm of A is the least possible number M such that `torch.norm(A@x) <= M*torch.norm(x)` so dividing A by this M insures you don't overflow. You can still vanish with this)\n",
    "\n",
    "\n",
    "- [Ref:Paper-Spectral Normalization](https://arxiv.org/pdf/1802.05957.pdf)\n",
    "\n",
    "\n",
    "## The magic number for scaling\n",
    "\n",
    "Here we will focus on the first one, which is the `Xavier initialization`. It tells us that we should use a scale equal to `1/math.sqrt(n_in)` [$\\frac{1}{\\sqrt{n_{IN}}}$] where `n_in` is the number of inputs of our matrix. [`n_in = inp_dim` i.e `input dimension`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  0.027958475053310394, std: 1.450333833694458\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512) / math.sqrt(512)\n",
    "\n",
    "for i in range(100): x = a @ x\n",
    "    \n",
    "stats(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.044194173824159216"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/math.sqrt(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed it works. Note that this magic number isn't very far from the 0.01 we had earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But where does it come from? It's not that mysterious if you remember the definition of the matrix multiplication. When we do `y = a @ x`, the coefficients of y are defined by\n",
    "\n",
    "$$y_{i} = a_{i,0} x_{0} + a_{i,1} x_{1} + \\cdots + a_{i,n-1} x_{n-1} = \\sum_{k=0}^{n-1} a_{i,k} x_{k}\n",
    "$$\n",
    "\n",
    "\n",
    "or in code:\n",
    "\n",
    "```py\n",
    "y[i] = sum([c*d for c,d in zip(a[i], x)])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now at the very beginning, our x vector has a mean of roughly 0. and a standard deviation of roughly 1. (since we picked it that way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  -0.05562354251742363, std: 1.0106923580169678\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "stats(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This is why it's extremely important to normalize your inputs in Deep Learning, the initialization rules have been designed with inputs that have a mean 0. and a standard deviation of 1.\n",
    "\n",
    "If you need a refresher from your statistics course, the mean is the sum of all the elements divided by the number of elements (a basic average). The standard deviation shows whether the data points stay close to the mean or are far away from it. It's computed by the following formula:\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{1}{n}\\left[(x_{0}-\\mu)^{2} + (x_{1}-\\mu)^{2} + \\cdots + (x_{n-1}-\\mu)^{2}\\right]}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go back to `y = a @ x` and assume that we chose weights for a that also have a mean of 0, we can compute the variance of y quite easily. Since it's random, and we may fall on bad numbers, we repeat the operation 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.019350024610757827, 511.56888916015623)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean,sqr = 0.,0.\n",
    "for i in range(100):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512, 512)\n",
    "    y = a @ x\n",
    "    mean += y.mean().item()\n",
    "    sqr  += y.pow(2).mean().item()\n",
    "mean/100,sqr/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that looks very close to the dimension of our matrix 512. And that's no coincidence! When you compute `y`, you sum 512 product of one element of `a` by one element of `x`. So what's the mean and the standard deviation of such a product of one element of `a` by one element of `x`? We can show mathematically that as long as the elements in `a` and the elements in `x` are independent, the mean is 0 and the std is 1.\n",
    "\n",
    "\n",
    "This can also be seen experimentally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0034053195815345588, 1.0258367947839448)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean,sqr = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(1)\n",
    "    a = torch.randn(1)\n",
    "    y = a*x\n",
    "    mean += y.item()\n",
    "    sqr  += y.pow(2).item()\n",
    "mean/10000,math.sqrt(sqr/10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we sum 512 of those things that have a mean of zero, and a variance of 1, so we get something that has a mean of 0, and variance of 512. To go to the standard deviation, we have to add a square root, hence math.sqrt(512) being our magic number.\n",
    "\n",
    "If we scale the weights of the matrix a and divide them by this math.sqrt(512), it will give us a y of scale 1, and repeating the product as many times as we want and it won't overflow or vanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding ReLU in the mix\n",
    "\n",
    "\n",
    "We can reproduce the previous experiment with a ReLU, to see that this time, the mean shifts and the variance becomes `0.5`. This time the magic number will be `math.sqrt(2/512)` to properly scale the weights of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3124287480063653, 0.4874404741692772)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean,sqr = 0.,0.\n",
    "\n",
    "def relu(x):\n",
    "    if x<0: return 0\n",
    "    else: return x.item()\n",
    "    \n",
    "for i in range(10000):\n",
    "    x = torch.randn(1)\n",
    "    a = torch.randn(1)\n",
    "    y = a*x\n",
    "    \n",
    "    # applying relu\n",
    "    y = relu(y) \n",
    "    mean += y\n",
    "    sqr  += y ** 2\n",
    "mean/10000,sqr/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can double check by running the experiment on the whole matrix product. The variance becomes 512/2 this time:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.145401511192322, 261.2793881225586)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean,sqr = 0.,0.\n",
    "for i in range(100):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512, 512)\n",
    "    y = a @ x\n",
    "    y = y.clamp(min=0)\n",
    "    mean += y.mean().item()\n",
    "    sqr  += y.pow(2).mean().item()\n",
    "mean/100,sqr/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or that scaling the coefficient with the magic number gives us a scale of 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5624650683999062, 0.9974158514950515)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean,sqr = 0.,0.\n",
    "for i in range(100):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512, 512) * math.sqrt(2/512)\n",
    "    y = a @ x\n",
    "    y = y.clamp(min=0)\n",
    "    mean += y.mean().item()\n",
    "    sqr  += y.pow(2).mean().item()\n",
    "mean/100,math.sqrt(sqr/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What exactly is Xavier initialization?\n",
    "\n",
    "Assigning the network weights before we start training seems to be a random process, right? We don’t know anything about the data, so we are not sure how to assign the weights that would work in that particular case. One good way is to assign the weights from a Gaussian distribution. Obviously this distribution would have zero mean and some finite variance. Let’s consider a linear neuron:\n",
    "\n",
    "$$y = w_1 x_1 + w_2 x_2 + \\dots + w_N x_N + b$$\n",
    "\n",
    "With each passing layer, we want the variance to remain the same. This helps us keep the signal from exploding to a high value or vanishing to zero. In other words, we need to initialize the weights in such a way that the variance remains the same for x and y.\n",
    "\n",
    "\n",
    "## How to perform Xavier initialization?\n",
    "\n",
    "Just to reiterate, we want the variance to remain the same as we pass through each layer. Let’s go ahead and compute the variance of y:\n",
    "\n",
    "$$var(y) = var(w_1 x_1 + w_2 x_2 + \\dots + w_N x_N + b)$$\n",
    "\n",
    "Let’s compute the variance of the terms inside the parentheses on the right hand side of the above equation. If you consider a general term, we have:\n",
    "\n",
    "$$var(w_i x_i) = E(x_i)^2var(w_i) + E(w_i)^2var(x_i) + var(w_i)var(x_i)$$\n",
    "\n",
    "Here, $E()$ stands for `expectation` of a given variable, which basically represents the mean value. We have assumed that the _inputs and weights are coming_ from a `Gaussian distribution` of zero mean. Hence the `E()` term vanishes and we get:\n",
    "\n",
    "$$var(w_i x_i) = var(w_i)var(x_i)$$\n",
    "\n",
    "Note that `b` is a constant and has zero variance, so it will vanish. Let’s substitute in the original equation:\n",
    "\n",
    "$$var(y) = var(w_1)var(x_1) + \\dots + var(w_N)var(x_N)$$\n",
    "\n",
    "Since they are all identically distributed, we can write:\n",
    "\n",
    "$$var(y) = N * var(w_i) * var(x_i)$$\n",
    "\n",
    "So if we want the variance of y to be the same as x, then the term $N * var(w_i)$ should be equal to `1`. Hence:\n",
    "\n",
    "$$N * var(w_i) = 1$$\n",
    "$$var(w_i) = \\frac{1}{N}$$\n",
    "\n",
    "There we go! We arrived at the **Xavier initialization** formula. We need to pick the weights from a Gaussian distribution with `zero mean` and a variance of $\\frac{1}{N}$, where N specifies the `inp_dim` i.e number of input neurons. This is how it’s implemented in the Caffe library. In the original paper, the authors take the average of the number input neurons and the output neurons. So the formula becomes:\n",
    "\n",
    "$$var(w_i) = \\frac{1}{N_{avg}}$$ \n",
    "\n",
    "where \n",
    "\n",
    "$$N_{avg} = \\frac{N_{in} + N_{out}}2{}$$\n",
    "\n",
    "The reason they do this is to preserve the backpropagated signal as well. But it is more computationally complex to implement. Hence we only take the number of input neurons during practical implementation.\n",
    "\n",
    "\n",
    "- [Blog: Xavier Init](https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/)\n",
    "\n",
    "- [Paper: Xavier Init](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "- [Paper: Kaiming Init](https://arxiv.org/abs/1502.01852)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
